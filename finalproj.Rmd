--- 
title: "Final project proposal"
author: "Bowen Dong"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
site: bookdown::bookdown_site
---
```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

# Proposal

## The data:
Airline Passenger Satisfaction from kaggle
https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction


Variables:
Gender: Gender of the passengers (Female, Male)

Customer Type: The customer type (Loyal customer, disloyal customer)

Age: The actual age of the passengers

Type of Travel: Purpose of the flight of the passengers (Personal Travel, Business Travel)

Class: Travel class in the plane of the passengers (Business, Eco, Eco Plus)

Flight distance: The flight distance of this journey

Inflight wifi service: Satisfaction level of the inflight wifi service (0:Not Applicable;1-5)

Departure/Arrival time convenient: Satisfaction level of Departure/Arrival time convenient

Ease of Online booking: Satisfaction level of online booking

Gate location: Satisfaction level of Gate location

Food and drink: Satisfaction level of Food and drink

Online boarding: Satisfaction level of online boarding

Seat comfort: Satisfaction level of Seat comfort

Inflight entertainment: Satisfaction level of inflight entertainment

On-board service: Satisfaction level of On-board service

Leg room service: Satisfaction level of Leg room service

Baggage handling: Satisfaction level of baggage handling

Check-in service: Satisfaction level of Check-in service

Inflight service: Satisfaction level of inflight service

Cleanliness: Satisfaction level of Cleanliness

Departure Delay in Minutes: Minutes delayed when departure

Arrival Delay in Minutes: Minutes delayed when Arrival

Satisfaction: Airline satisfaction level(Satisfaction, neutral or dissatisfaction)


## Modeling goal:
This dataset contains an airline passenger satisfaction survey.We want to know what factors are highly correlated to a satisfied (or dissatisfied) passenger, and try to model and predict passenger satisfaction.



## The models we intend to use: 
random forest(white box), lightGBM(black box), KNN(black box), logistic regression(white box)

<!--chapter:end:index.Rmd-->

---
title: "EDA"
output: html_document
date: "2023-04-18"
---
```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Data science
library(tidyverse)
library(DataExplorer)
library(skimr)
library(lubridate)


# Modeling
library(tidymodels)
library(caret)

# Visualization
library(ggrepel)
library(GGally)
library(vip)
library(patchwork)
library(ggplot2)
library(corrplot)

# Tables
library(gt)


```

```{r}
train <- read.csv("train.csv",stringsAsFactors = F)
test <- read.csv("test.csv",stringsAsFactors = F)
options(scipen=999) # erase 'e' in number
options(warn = -1)

```
```{r}

# Train data
plot_missing(train,title='NA value in Train dataset')

#Test data
plot_missing(test,title='NA value in Test dataset')


```
```{r}
miceresult <- mice::mice(train,seed=123,m=5)#the m is the number of divide
miceresult
train <- mice::complete(miceresult,1) #remove the missing values in the train set
miceresult <- mice::mice(test,seed=123,m=5)
miceresult
test <- mice::complete(miceresult,1)#remove the missing values in the test set
```

```{r}
train_rf=train
test_rf=test
```

```{r}
age1<- ggplot(data = train, aes( x = Age)) + 
  geom_density(fill="lightblue", color="black", alpha=0.8) +
  labs(title = "Distribution of Age", 
       subtitle = "Airline Passenger Satisfaction, Kaggle(2021.08)")

age2 <- train %>%
  mutate(satisfaction = recode(satisfaction, `0` = "neutral or dissatisfied", `1` = "satisfied")) %>%
  ggplot(aes(x = Age, fill = satisfaction)) +
  geom_density(alpha = 0.5) +
  labs(x = NULL) +
  theme_bw() +
  facet_wrap(~satisfaction)

age1/age2

#flight distance histogram
flight1<- ggplot(data = train, aes( x = Flight.Distance)) + 
  geom_density(fill="lightblue", color="black", alpha=0.8) +
  labs(title = "Distribution of Flight.Distance", 
       subtitle = "Airline Passenger Satisfaction, Kaggle(2021.08)")

flight2 <- train %>%
  mutate(satisfaction = recode(satisfaction, `0` = "neutral or dissatisfied", `1` = "satisfied")) %>%
  ggplot(aes(x = Flight.Distance, fill = satisfaction)) +
  geom_density(alpha = 0.5) +
  labs(x = NULL) +
  theme_bw() +
  facet_wrap(~satisfaction)

flight1/flight2


```

```{r}
#Arrival.Delay.in.Minutes
arrive1<- ggplot(data = train, aes( x = Arrival.Delay.in.Minutes)) + 
  geom_density(fill="lightblue") +
  xlim(0, 75) +
  labs(title = "Distribution of Arrival.Delay.in.Minutes", 
       subtitle = "Airline Passenger Satisfaction, Kaggle(2021.08)")

arrive2 <- train %>%
  mutate(satisfaction = recode(satisfaction, `0` = "neutral or dissatisfied", `1` = "satisfied")) %>%
  ggplot(aes(x = Arrival.Delay.in.Minutes, fill = satisfaction)) +
  geom_density(alpha = 0.5) +
  xlim(0, 75) +
  labs(x = NULL) +
  theme_bw() +
  facet_wrap(~satisfaction)

arrive1/arrive2

#Departure.Delay.in.Minutes
depart1<- ggplot(data = train, aes( x = Departure.Delay.in.Minutes)) + 
  geom_density(fill="lightblue") +
  xlim(0, 75) +
  labs(title = "Distribution of Departure.Delay.in.Minutes", 
       subtitle = "Airline Passenger Satisfaction, Kaggle(2021.08)")

depart2 <- train %>%
  mutate(satisfaction = recode(satisfaction, `0` = "neutral or dissatisfied", `1` = "satisfied")) %>%
  ggplot(aes(x = Departure.Delay.in.Minutes, fill = satisfaction)) +
  geom_density(alpha = 0.5) +
  xlim(0, 75) +
  labs(x = NULL) +
  theme_bw() +
  facet_wrap(~satisfaction)

depart1/depart2
#Check the distribution of numerical features

```

```{r}
train %>% 
  dplyr::select(,9:16) %>% 
  plot_histogram(theme_config = theme_bw(), 
                 geom_histogram_args = list(fill = "salmon"),title = "Scored Variables distirbution in trianset(1)")

train %>% 
  dplyr::select(,17:22) %>% 
  plot_histogram(theme_config = theme_bw(), 
                 geom_histogram_args = list(fill = "lightblue"),title = "Scored Variables distirbution in trianset(2)")
#Check distribution of the score variables. Those are more categorical than numerical

```

```{r}
c5 <- train %>% 
  mutate(satisfaction = recode(satisfaction, `0` = "neutral or dissatisfied", `1` = "satisfied")) %>%
  count(satisfaction) %>% 
  ggplot(aes(x = satisfaction, y = n, fill = satisfaction)) +
  geom_bar(stat = "identity", colour = "black", alpha = 0.75) +
  geom_text(aes(label = n), position=position_dodge(width=0.9), vjust=-0.5) +
  ylim(0, 60000) +
  labs(y = "Count") +
  theme_bw()  + ggtitle("Sactisfaction variable in Train Dataset")

c1 <-train %>% 
  mutate(Gender = recode(Gender, `0` = "Female", `1` = "Male")) %>%
  count(Gender) %>% 
  ggplot(aes(x = Gender, y = n, fill = Gender)) +
  geom_bar(stat = "identity", colour = "black", alpha = 0.75) +
  geom_text(aes(label = n), position=position_dodge(width=0.9), vjust=-0.5) +
  ylim(0, 60000) +
  labs(y = "Count") +
  theme_bw()  + ggtitle("Gender variable in Train Dataset")

c2 <- train %>% 
  mutate(Customer.Type = recode(Customer.Type, `0` = "disloyal Customer", `1` = "Loyal Customer")) %>%
  count(Customer.Type) %>% 
  ggplot(aes(x = Customer.Type, y = n, fill = Customer.Type)) +
  geom_bar(stat = "identity", colour = "black", alpha = 0.75) +
  geom_text(aes(label = n), position=position_dodge(width=0.9), vjust=-0.5) +
  ylim(0, 100000) +
  labs(y = "Count") +
  theme_bw() + ggtitle("Customer.Type variable in Train Dataset")

c3 <- train %>% 
  mutate(Type.of.Travel = recode(Type.of.Travel, `0` = "Business travel", `1` = "Personal Travel")) %>%
  count(Type.of.Travel) %>% 
  ggplot(aes(x = Type.of.Travel, y = n, fill = Type.of.Travel)) +
  geom_bar(stat = "identity", colour = "black", alpha = 0.75) +
  geom_text(aes(label = n), position=position_dodge(width=0.9), vjust=-0.5) +
  ylim(0, 100000) +
  labs(y = "Count") +
  theme_bw()  + ggtitle("Type.of.Travel variable in Train Dataset")

c4 <- train %>% 
  mutate(Class = recode(Class, `0` = "Business", `1` = "Eco",`2` = "Eco Plus")) %>%
  count(Class) %>% 
  ggplot(aes(x = Class, y = n, fill = Class)) +
  geom_bar(stat = "identity", colour = "black", alpha = 0.75) +
  geom_text(aes(label = n), position=position_dodge(width=0.9), vjust=-0.5) +
  ylim(0, 100000) +
  labs(y = "Count") +
  theme_bw() + ggtitle("Class variable in Train Dataset")

c1/c2
c3/c4
c5
#Here we display histogram to demonstrate distribution of character variables. We find that the genders of customers are quite balanced. Majority of the travels are business travels. There are more neutral or dissatisfied reviews than satisfied, but not by a large margin. 

```





```{r}
#Correlation matrix
df_cor2 <- cor(select_if(subset(train, select=-c(id, Age, Flight.Distance,Departure.Delay.in.Minutes,Arrival.Delay.in.Minutes)), is.numeric))
summary(df_cor2)
options(repr.plot.width = 14, repr.plot.height = 8)
corrplot(df_cor2, na.label=" ", tl.cex=1, tl.col="black", method="color")
```


<!--chapter:end:EDA.rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# [Model 1]

At first, we approach with a logistic regression. 
```{r}
library(broom)
library(tidyverse)
library(caret)
library(MASS)
```
```{r}
train$Inflight.wifi.service = as.factor(train$Inflight.wifi.service)
train$Departure.Arrival.time.convenient = as.factor(train$Departure.Arrival.time.convenient)
train$Ease.of.Online.booking = as.factor(train$Ease.of.Online.booking) 
train$Gate.location = as.factor(train$Gate.location)
train$Food.and.drink = as.factor(train$Food.and.drink)
train$Online.boarding = as.factor(train$Online.boarding)
train$Seat.comfort = as.factor(train$Seat.comfort)
train$Inflight.entertainment = as.factor(train$Inflight.entertainment)
train$On.board.service = as.factor(train$On.board.service)
train$Leg.room.service = as.factor(train$Leg.room.service)
train$Baggage.handling = as.factor(train$Baggage.handling)
train$Checkin.service = as.factor(train$Checkin.service)
train$Inflight.service = as.factor(train$Inflight.service)
train$Cleanliness = as.factor(train$Cleanliness)
train$satisfaction  = as.factor(train$satisfaction )
```

```{r}
test$Inflight.wifi.service = as.factor(test$Inflight.wifi.service)
test$Departure.Arrival.time.convenient = as.factor(test$Departure.Arrival.time.convenient)
test$Ease.of.Online.booking = as.factor(test$Ease.of.Online.booking) 
test$Gate.location = as.factor(test$Gate.location)
test$Food.and.drink = as.factor(test$Food.and.drink)
test$Online.boarding = as.factor(test$Online.boarding)
test$Seat.comfort = as.factor(test$Seat.comfort)
test$Inflight.entertainment = as.factor(test$Inflight.entertainment)
test$On.board.service = as.factor(test$On.board.service)
test$Leg.room.service = as.factor(test$Leg.room.service)
test$Baggage.handling = as.factor(test$Baggage.handling)
test$Checkin.service = as.factor(test$Checkin.service)
test$Inflight.service = as.factor(test$Inflight.service)
test$Cleanliness = as.factor(test$Cleanliness)
test$satisfaction  = as.factor(test$satisfaction )
```


```{r}
set.seed(5293)
mod_logit1 <- glm(satisfaction ~ Gender + Customer.Type + Age + 
                 Type.of.Travel + Class + Flight.Distance + Inflight.wifi.service + 
                 Departure.Arrival.time.convenient + Ease.of.Online.booking + 
                 Gate.location + Food.and.drink + Online.boarding + Seat.comfort +
                 Inflight.entertainment + On.board.service + Leg.room.service +
                 Baggage.handling + Checkin.service + Inflight.service +
                 Cleanliness + Departure.Delay.in.Minutes + Arrival.Delay.in.Minutes , data = train, family = "binomial")
```
```{r}
summary(mod_logit1)

```
```{r}
#Check the effects of departure delay since it exhibits strange behavior
library(effects)
plot(Effect(focal.predictors = c("Departure.Delay.in.Minutes"), mod = mod_logit1))
```

```{r}
#Refit with significant variables
mod_logit2 <- glm(satisfaction ~ Gender + Customer.Type + Age + 
                 Type.of.Travel + Class + Flight.Distance + Inflight.wifi.service + 
                 Departure.Arrival.time.convenient + Ease.of.Online.booking + 
                 Gate.location + Food.and.drink + Online.boarding + Seat.comfort +
                 Inflight.entertainment + On.board.service + Leg.room.service +
                 Baggage.handling + Checkin.service + Inflight.service +
                 Cleanliness + Departure.Delay.in.Minutes  , data = train, family = "binomial")
```

```{r}
summary(mod_logit2)
#After removing the arrival delay feature, we find the depature delay variable behaves as what supposed
```

```{r}
anova(mod_logit1, mod_logit2, test="Chisq")
#Mod 1 is better
```


```{r}
pred <- predict(mod_logit2, type = "response", newdata = test)
confusion_matrix <- table(test$satisfaction, pred>0.7)
row.names(confusion_matrix)<- c("neutral or dissatisfied", "satisfied")
noquote("Confusion Matrix")
confusion_matrix
accuracy=(confusion_matrix[1]+confusion_matrix[4])/25976
accuracy
```
```{r}
varImp(mod_logit1, scale = FALSE)

```





<!--chapter:end:model1.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# [Model 2]
Random Forest
```{r}
set.seed(5293)
library(party)
library(randomForest)
```


```{r}
mod_rf=randomForest(satisfaction ~ Gender + Customer.Type + Age + Type.of.Travel + 
    Class + Flight.Distance + Inflight.wifi.service + Departure.Arrival.time.convenient + 
    Ease.of.Online.booking + Gate.location + Food.and.drink + 
    Online.boarding + Seat.comfort + Inflight.entertainment + 
    On.board.service + Leg.room.service + Baggage.handling + 
    Checkin.service + Inflight.service + Cleanliness + Departure.Delay.in.Minutes + 
    Arrival.Delay.in.Minutes, data=train_rf, controls=cforest_control(mtry=2, mincriterion=0))
```

```{r}
pred=predict(mod_rf,test_rf,type='class')
table(pred,test_rf$satisfaction)
accuracy=mean(pred==test$satisfaction)
accuracy
```

```{r}
print(mod_rf)
```
```{r}
varImpPlot(mod_rf)
```




<!--chapter:end:model2.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# [Model 3]
We are going to set up and try catboost

```{r}
library(catboost)
```


```{r}
set.seed(5293)
train <- read.csv("train.csv",stringsAsFactors = F)
test <- read.csv("test.csv",stringsAsFactors = F)
options(scipen=999) # erase 'e' in number
options(warn = -1)

```
```{r}

# Train data
plot_missing(train,title='NA value in Train dataset')

#Test data
plot_missing(test,title='NA value in Test dataset')


```
```{r}
miceresult <- mice::mice(train,seed=123,m=5)#the m is the number of divide
miceresult
train <- mice::complete(miceresult,1) #remove the missing values in the train set
miceresult <- mice::mice(test,seed=123,m=5)
miceresult
test <- mice::complete(miceresult,1)#remove the missing values in the test set
```

```{r}
train <- train[,-c(1,2)] #delete X, id feature
test=test[,-c(1,2)]
train[,-c(1,2,4,5,23)] <- scale(train[,-c(1,2,4,5,23)]) # scale the 'Gender','Customer.Type','Type.of.Travel','Class','satisfaction' variables
test[,-c(1,2,4,5,23)] <- scale(test[,-c(1,2,4,5,23)])
```

```{r}
train$Gender <- factor(train$Gender)
train$Customer.Type <- factor(train$Customer.Type)
train$Type.of.Travel <- factor(train$Type.of.Travel)
train$Class <- factor(train$Class)
train$satisfaction <- factor(train$satisfaction)

test$Gender <- factor(test$Gender)
test$Customer.Type <- factor(test$Customer.Type)
test$Type.of.Travel <- factor(test$Type.of.Travel)
test$Class <- factor(test$Class)
test$satisfaction <- factor(test$satisfaction)

```

```{r}
# set x and y 
x <- train[,-23]
y <- train$satisfaction

# use caret for grid search 
fit_control <- caret::trainControl(
  method = "cv", 
  number = 3, 
  search = "random",
  classProbs = TRUE
)

# set grid options
grid <- expand.grid(
  depth = c(4, 6, 8),
  learning_rate = 0.1,
  l2_leaf_reg = 0.1,
  rsm = 0.95,
  border_count = 64,
  iterations = c(100,200,300)
)

# train catboost
mod_cat <- train(
  x = x, 
  y = as.factor(make.names(y)),
  method = catboost.caret,
  logging_level = 'Verbose',
  maximize = TRUE,
  preProc = NULL,
  tuneGrid = grid, 
  tuneLength = 30, 
  trControl = fit_control
)

```

```{r}
print(mod_cat)
```

```{r}
coef <- varImp(mod_cat, scale = FALSE)
pred <- predict(mod_cat, test[,-23]) %>% as.data.frame() %>% rename('predict'='.')
pred$predict <- gsub("neutral.or.dissatisfied", "neutral or dissatisfied",pred$predict)
pred$predict <- factor(pred$predict)

answer <- test[,23] %>% as.data.frame() %>% rename('answer'='.')
answer$answer <- factor(answer$answer)
```
```{r}
catboost_confm <- confusionMatrix(pred$predict, answer$answer,positive = 'satisfied')
catboost_confm
```


```{r}
plot(coef)
```


<!--chapter:end:model3.Rmd-->

```{r include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)
```
# Reflections
  Overall, We have tried three models: Logistic Regression, Random Forest and Catboost, ordering from most interpretable and least interpretable. Logistic Regression has an accuracy of  0.9248, while random forest has an accuracy of 0.963, and Catboost, the best model in terms of accuracy, has an accuracy of  0.964 .
  In our data pre-processing, we have utilized the 'mice' package to impute the missing values, and scaled some of the numerical variables to normal. We also converted the categorical variables to factors. The mice package shows great capability of detecting and imputing empty values. 
  In our logistic regression model, we have detected that the departure delay variable is showing unusual pattern. It is extremely unlikely that customers will feel more satisfied when airlines departs late; However our result shows such. We suspect the existence of Simpson's paradox; But we can not confirm it since we do not know if our data is a subset of a larger population. Removing either the departure delay or the arrival delay makes it behaving normally. We can see that since the departure delay is a confounding variable for both Arrival delay and satisfaction: If a plane departs late, it is very likely to arrive late. It is likely that the confounding effect from the departure delay variable caused its unnatural results. 
  The random forest model and catboost model have very close accuracies. Both machine learning models are less interpretable. Although their accuracies are very close, their features are more different. For Catboost, wifi service has really high importance, and then type of travel and online-boarding. For random forest, online-boarding is the highest, following wifi-service and class. The big picture of variable importance are similar for both machine learning models, with some differences.
  It is very different for logistic regression. The top three variables with most importance are type of travel, customer type and checkin service, which is vastly different from the machine learning models. 
  


<!--chapter:end:reflections.Rmd-->

